---
layout: post
title: 分布式一致性哈希算法原理
tags:
- 分布式系统
categories: distribute-systems
description: 分布式系统理论
---

本文我们介绍一下分布式一致性哈希算法的基本原理。

<!-- more -->


## 1. 分布式算法

在做服务器负载均衡的时候，可供选择的负载均衡的算法有很多，包括： 轮询算法（Round Robin)、哈希算法（Hash）、最少连接算法（Least Connection）、响应速度算法(Response Time)、加权法(Weighted)等。其中哈希算法是最为常用的算法。

典型的应用场景是： 有N台服务器提供缓存服务，需要对服务器进行负载均衡，将请求平均分发到每台服务器上，每台机器负责1/N的服务。

常用的算法是对hash结果取余数(hash() mod N): 对机器编号从0到N-1，按照自定义的hash()算法，对每个请求的hash()值按N取模，得到```余数i```，然后将请求分发到编号为```i```的机器。但这样的算法存在致命问题，如果某一台机器宕机，那么应该落在该机器上的请求就无法得到正确的处理，这时需要将宕掉的服务器从算法中移除，此时会有```(N-1)/N```的服务器的缓存数据需要重新计算；如果新增一台机器，会有```N/(N+1)```的服务器的缓存数据需要进行重新计算。对于系统而言，这通常是不可接受的颠簸（因为这意味着大量缓存的失效或者数据需要转移）。那么，如何设计一个负载均衡策略，使得受到影响的请求尽可能的少呢？

在Memcached、Key-Value Store、Bittorrent DHT、LVS中都采用了Consistent Hashing算法，可以说Consistent Hashing是分布式系统负载均衡的首选算法。


## 2. 分布式缓存问题
在大型web应用中，缓存可以算是当今的一个标准开发配置了。在大规模的缓存应用中，应运而生了分布式缓存系统。分布式缓存系统的基本原理，大家也有所耳闻。key-value如何均匀的分散到集群中？ 说到此，最常规的方式莫过于hash取模的方式。比如集群中可用机器数量为```N```，那么key值为```K```的数据请求很简单的应该路由到```hash(K) mod N```对应的机器。的确，这种结构是简单的，也是实用的。但是在一些高速发展的Web系统中，这样的解决方案仍有些缺陷。随着系统访问压力的增长，缓存系统不得不通过增加机器节点的方式提高集群的响应速度和数据承载量。增加机器意味着按照hash取模的方式，在增加机器节点的这一时刻，大量的缓存命不中，缓存数据需要重新建立，甚至是进行整体的缓存数据迁移，瞬间会给DB带来极高的系统负载，甚至导致DB服务器宕机。那么就没有办法解决hash取模的方式带来的诟病吗？


假设我们有一个网站，最近发现随着流量的增加，服务器压力越来越大，之前直接读写数据库的方式不太给力了，于是我们想引入```Memcached```作为缓存机制。现在我们一共有三台机器可以作为Memcached服务器，如下图所示：


![hc-memcached](https://ivanzz1001.github.io/records/assets/img/distribute/hc_memcached_system.png)

很显然，最简单的策略是将每一次Memcached请求随机发送到一台Memcached服务器，但是这种策略可能会带来两个问题： 一是同一份数据可能被存在不同的机器上而造成数据冗余； 二是有可能某数据已经被缓存但是访问却没有命中，因为无法保证对相同key的所有访问都被发送到相同的服务器。

要解决上述问题只需做到如下一点： 保证对相同key的访问会被发送到相同的服务器。很多方法可以实现这一点，最常用的方法是计算哈希。例如对于每次访问，可以按如下算法计算其哈希值：
{% highlight string %}
h = Hash(key) % 3
{% endhighlight %}

其中Hash是从一个字符串到正整数的哈希映射函数。这样，如果我们将Memcached Server分别编号为0、1、2，那么就可以根据上式和key计算出服务器编号```h```，然后去访问。

这个方法虽然解决了上面提到的两个问题，但是存在一些其它的问题。如果将上述方法抽象，可以认为通过：
{% highlight string %}
h = Hash(key) % N
{% endhighlight %}}
这个算式计算每个key的请求应该被发送到哪台服务器，其中```N```为服务器的台数，并且服务器按照```0 ~ (N-1)```编号。
 

这个算法的问题在于容错性和扩展性不好。所谓容错性是指当系统中某一个或几个服务器变得不可用时，整个系统是否可以正确高效运行； 而扩展性是指当加入新的服务器后，整个系统是否可以正确高效运行。


现假设有一台服务器宕机了，那么为了填补空缺，要将宕机的服务器从编号列表中移除，后面的服务器按顺序前移一位并将其编号值```减1```，此时每个key就要按 **h = Hash(key) % (N-1)** 重新计算； 同样，如果新增了一台服务器，虽然原有服务器编号不用改变，但是要按 **h = Hash(key) % (N+1)** 重新计算哈希值。因此系统中一旦有服务器变更，大量的key会被重定位到不同的服务器，从而造成大量的缓存不能命中。这这种情况在分布式系统中是非常糟糕的。

一个设计良好的分布式哈希方案应该具有良好的单调性，即服务节点的增减不会造成大量哈希重定位。一致性哈希算法就是这样一种哈希方案。

Hash算法的一个很亮指标是```单调性```(Monotonicity)，定义如下：单调性是指如果已经有一些内容通过哈希分配到了相应的缓冲中，又有新的缓冲加入到系统中，哈希的结果应该能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。


容易看到，上面的简单hash算法** hash(object) % N**难以满足单调性要求。

## 3. 一致性Hash算法背景

一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。

但现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是又客户端来提供，具体在计算一致性hash时采用如下步骤：

1) 首先求出memcached服务器（节点）的哈希值，并将其配置到```0~2^32```的圆(continuum)上；

2) 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上；

3） 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过2^32仍找不到服务器，就会保存到第一台memcached服务器上。

![hc-figure-1](https://ivanzz1001.github.io/records/assets/img/distribute/hc_figure_1.png)












<br />
<br />

**[参看]**

1. []()

2. []()

<br />
<br />
<br />


