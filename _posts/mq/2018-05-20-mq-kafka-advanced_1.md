---
layout: post
title: kafka进阶
tags:
- mq
categories: mq
description: kafka进阶
---

前面我们对于kafka的介绍主要还是偏向于使用操作层面，这里作为kafka进阶的第一篇，我们讲述一下kafka若干方面的设计原理。



<!-- more -->

## 1. kafka设计动机
我们设计kafka的主要目的是为了能够有一个统一的平台(platform)来处理各种大规模的实时流数据。为了实现这一目标，我们在设计时可能就得考虑各种用例场景:

* kafka必须拥有高吞吐率，以支持大量的实时流数据，例如日志集

* kafka需要能够优雅的处理大量的日志数据，以支持某一时刻数据从离线系统进行加载

* kafka在处理消息时必须具备低延迟

因此，我们希望创建的kafka能够支持分区(partitioned)、分布式(distributed)、实时(real time)的来处理输入的数据，然后产生新的输出。这就是kafka的```分区模型```(partitioning model)以及```消费模型```(consumer model)的设计由来。

最后，由于输入到kafka中的数据流所产生的输出可能还需要提供给其他系统使用，因此系统必须要能够保证高容错性以应对机器可能出现的宕机情况。


为了实现上述这样一些目标，我们就得考虑方方面面，而不仅仅是在传统消息系统层面加上一个database log。

## 2. 持久化

### 2.1 不要畏惧文件系统
kafka严重依赖文件系统来存储和缓存消息，而我们通常有一个观念会认为“磁盘是低速的”，因此就会怀疑使用磁盘来存储数据到底能不能够达到一个有竞争力的性能指标。实际上，在使用磁盘时主要取决于使用者的期望，如果我们期望很高，则磁盘可能确实是一个很低速的设备，反之则其实磁盘性能也未必有那么差。如果我们适当的设计数据在磁盘上的存储结构，则很可能能够获得与网络(network)相同数量级的速度。

在过去十年，有关磁盘性能的关键事实是，硬盘驱动器的吞吐量与磁盘寻道的延迟时间是不相同的。如果我们在一个JBOD配置，转速为7200rpm的SATA RAID-5磁盘矩阵上进行连续写入的话，每秒钟的写入速度可以达到600MB；而如果我们采用随机写的话，则每秒钟的写入速度可能只有100KB，这两者之间相差了6000倍。在所有的使用模式当中，线性的读、写操作均是最优的，并且得到了操作系统的深度优化。现代操作系统通常都提供了预读(pre-read)技术来处理大块数据读取，也提供了延迟写(write-behind)技术来将将多个逻辑写操作合并成一个物理写操作。


为了弥补磁盘性能方面的不足，现代操作系统开始大量使用内存来为磁盘做缓存。现代OS通常都倾向于使用大量空闲内存空间来作为磁盘缓存，而当内存需要收回时，对性能的损耗也是微乎其微的。磁盘所有的读写操作都会通过操作系统这一统一的缓存来进行。该特性（通过操作系统缓冲读写磁盘文件）通常很难被关闭，除非使用direct IO，因此即使一个进程在进程内维持着自己的一份数据缓存，该数据也会在OS pagecache再缓存一次，这样就可能会导致缓存两次。

更重要的是，因为Kafka是构建在JVM之上的，所有了解Java内存使用的人都应该知道：

* 所占用的内存通常会大大超出Java objects的大小，很多时候会超过其2倍，甚至更多

* 随着堆(heap)中对象数据的增加，java垃圾回收会开始变的低速

正是由于上面这些原因，我们更倾向于使用文件系统和操作系统的pagecache，而不是自己在进程中维持一份缓存———这样至少增加了操作系统可用缓存。因此，在一个32GB内存的系统上就可以有多达28~30GB的空间来作为cache。更为重要的是，即使服务(service)重启，这份缓存可能仍然可以使用，而应用程序内的缓存则需要进行重建。同时，采用操作系统缓存也可以大大简化代码的编写，而不必担心缓存与文件系统之间数据的一致性逻辑。假如我们的读操作大部分都是线性读的话，则预读取(read-ahead)通常可以很好的预先把我们要从磁盘读取的数据先加载到cache中。


这就意味着我们的设计可以非常简单：我们并不需要在应用程序内存中缓存数据，然后在应用程序出现panic时flush到文件系统，取而代之的是我们只需要利用操作系统的pagecache就可以了。所有写到文件系统的持久化日志，我们通常都不会强制刷新到硬盘。实际上这部分写入的数据只是被发送到了操作系统内核的pagecache上。

### 2.2 常数时间读

在消息系统中，通常是每个consumer都各自用一个queue来维护消息的元数据，并将其作为持久化数据结构。该队列通常是一个关联BTree或者其他通用的随机访问数据结构。BTrees是一种多功能数据结构，这使得其能够在消息系统中广泛的支持各种事务(transaction)与非事务(non-transaction)功能实现。尽管BTree操作的时间复杂度为O(logN)，但其还是会有一个相对高的性能消耗。通常O(logN)会被认为是常数时间(constant time)，但对于磁盘操作来说这可能是不成立的。磁盘的寻址时间可能通常需要10ms，并且每一块磁盘并不能并发的进行寻址。因此，即使是一次常规的磁盘寻址延迟就会相当高。在一个存储系统中，通常高速的缓存操作与低速的磁盘操作是混杂在一起的，因此我们所观测到的树型结构的性能可能会随着数据量的增加出现线性的下降。

直观上来说，可以像常用的日志解决方案(logging solutions)那样，队列的持久化也可以做成简单的read和append文件操作。这种数据结构的优点在于所有的操作都可以在O(1)时间内完成，并且读写操作并不会相互阻塞。这具有明显的性能优势，因为其完全不会受到所存数据量的影响————单台服务器使用廉价的、低速的1+TB SATA硬盘就可满足要求。尽管这样的配置可能具有低效的寻址能力，然而对于大规模的数据读写却有可接受的性能，并且使用1/3的价格就可以获得3倍的容量大小。

通过访问虚拟磁盘空间，我们可以获得一个较高的性能，这就意味着kafka消息系统可以提供一些传统消息系统所没有的特性。例如，在kafka中，当消息被消费完之后，其并不会马上被删除，我们可能仍然会将数据保存很长一段时间。这就为消费者带来了很大的灵活性。

## 3. kafka效率
在提高kafka系统效率层面，我们做了很大的努力。kafka的主要用例之一就是处理web的active数据，这种数据通常数据量很大：每一次浏览web页面都可能产生大量的kafka写操作。更为重要的是，我们假设每一条发布的消息都至少会被一个consumer所消费，因此我们还力求整个系统尽量做到廉价。

在构建和运营一些相似的系统的过程中，我们还发现在多租户操作中，效率往往是一个关键因素。假设由于应用程序的某个小故障而使得数据的下行服务很容易成为瓶颈时，则这样的系统很可能会出现问题。这时候就要求kafka能够快速的为下游应用程序分担负载压力。这一点对于为数百个其他应用程序提供中心化服务的系统来说尤其重要。

在上一节我们已经讨论了磁盘的性能问题。一旦消除了不良的磁盘访问模式，这种系统的低效问题通常就只剩两种：大量的small IO操作， 以及过量的字节拷贝。

small IO的问题不仅发生在client与server之间，也发生在server本身的持久化操作上。

为了避免small IO，我们的协议构建于*message set*抽象之上，将消息打包组合在一起。这就允许将消息组合在一起，通过一个网络请求来进行处理。而服务器反过来也可以批量的将消息数据写入到日志中， consumer也可以批量的抓取数据。

这一小的优化大大提高了kafka的整体速度。但是```批量```(batching)会产生更大的网络数据包，更大的顺序磁盘操作，以及占用更多的连续内存块，然而却可以使得kafka将突发的随机消息流转换成线性写操作，然后供consumer来进行消费。

另一个低效的地方在于字节拷贝。在低速收发消息时，这可能不会存在问题，但是在高负荷之下则可能会对性能产生很大的影响。为了避免此问题，我们设计了一种标准的二进制协议，使得producer、broker、consumer均可共享使用此协议（这样在数据簇进行传输时，就可以避免相互之间的转换）。

broker所维护的消息日志其本身就仅仅是文件系统中某个目录下的一些文件，每个文件中保存了一系列的消息，保存的格式与producer、consumer中所用的消息格式一致。维持统一的格式，使得可以在一些重要的操作上进行优化：网络可以直接传输持久化的日志数据。现代Unix操作系统提供了一种高度优化的路径来实现pagecache与socket之间的数据传输，在Linux上可以使用sendfile()零拷贝来进行实现.


为了理解sendfile()的零拷贝，我们首先来看一下常规的文件发送流程:

1) 操作系统从硬盘中将数据读入内核空间的pagecache

2） 应用程序从内核空间将数据读到用户空间

3） 应用程序将数据写回到处于内核空间的socket buffer

4） 操作系统将socket buffer中的数据拷贝到NIC的buffer，然后发送到网络上

从上述过程来看，进行了4次数据拷贝和2次系统调用，这显然是十分低效的。使用sendfile()，可以避免数据的多次拷贝，允许将数据直接从pagecache发送到网络。在这一优化的传输路径中，第一次和最后一次数据拷贝是必要的。

一个常见的用例是，多个consumer同时消费一个topic。使用上面介绍的sendfile()零拷贝技术，数据只会拷贝到pagecache中，且只拷贝一次，多个consumer可以共享使用数据，这样就可以使得消息的消费速率能够达到网络带宽的限制。

通过综合使用pagecache与零拷贝技术，我们可能通常会发现差不多所有的consumer都会在一个相同的消费位置，并且几乎看不到磁盘的读操作，因为数据都是来源于pagecache。

### 2.4 端到端的批量压缩
很多时候可能系统的瓶颈确实不是来自于CPU，也不是来自于磁盘，而是来自于网络带宽。这特别容易发生在数据中心向外部网络发送消息的场景。当然，即使在没有kafka支持的情况下，也可以单独对消息进行压缩，然而这可能会有一个比较低的压缩率。高效的压缩通常需要我们将多条数据组合在一起，形成一个批量再进行压缩。

kafka支持高效的批量格式。批量的消息可以打包在一起进行压缩，然后发送到服务器。批量的消息数据也会直接以压缩的格式写入到日志文件，并且只会被consumer所解压。


kafka目前支持Gzip、Snappy、LZ4以及ZStandard压缩协议。

## 3. Producer

### 3.1 负载均衡
producer是直接向partition的Leader所在的broker发送消息数据的，而不会经过任何中间的路由层。为了帮助producer实现此目的，所有kafka节点都必须要能够响应producer获取元数据的请求———当前哪些服务器处于alive状态，某个topic的所有partitions所对应的leader地址是啥。

客户端控制将消息发送到哪个partition。这通常可以随机发往某个partition，也可以实现某种意义上的分区映射。我们暴露了相应的接口来实现分区映射：允许用户传递一个特殊的key，然后使用某种hash映射方法映射到一个具体的partition上（甚至我们可以重写自己的hash映射函数）。例如，假设我们选择的key是user id的话，则给定用户的所有数据都将会发送到同一个partition上。这反过来允许consumer做一些本地化的消费预测。

### 3.2 异步消息发送
kafka通过批量操作实现了巨大的效率提升，为了实现批量kafka producer将会尝试在内存中累积一定量的数据，然后将其组装成一个大的批量以实现在一个请求中发送。我们可以通过配置的方式来指定一个批量最大的字节数(比如64kb)，或者最长等待多长时间来获得一个批量(比如10ms)。这样就能够尽量的累积到足够的数据来发送，以及尽量的避免小的IO操作。这样通过引入一定的延迟，从而实现一个较高的吞吐量。



## 4. Consumer
kafka consumer通过向对应partition的Leader发送fetch请求来消费数据。在每一个请求中，consumer都会制定所要抓取的offset，然后会收到从该偏移位置开始的一簇数据。因此，consumer能够完全的控制所要消费的偏移位置。

### 4.1 Push vs. Pull
我们考虑的一个首要问题是：消费者主动从broker拉取数据，还是broker推送数据到consumer。在这一方面，kafka遵循大多数消息系统的设计方式，producer将数据push到broker，consumer从broker拉取数据。一些以日志为中心(logging-centric)的系统，比如Scribe和Apache Flume，这些系统是采用push的方式往下游推送数据的。其实，push或pull这两种方式各有优缺点。然而，基于push系统很难满足各式各样的用户需求，因为是由broker来控制数据的传输速率的。我们的目标通常是consumer能够能够达到最大的消费速率，然而不幸的是，在一个push系统中当consumer的消费速率较低时，大量的push数据就很可能会压垮整个系统。基于pull的系统通常就没有这方面的问题，consumer可以落后于broker，并在适当的时候追赶上broker。我们可以采用某种类型的补偿协议，使得consumer可以预测其是否会被压垮，然而要准确的获得consumer的最大消费速度其实比想象中的更为困难，因此在构建kafka时我们还是采用传统消息系统常用的pull模式。

基于pull模式的系统的另一个优点在于consumer自身可以控制是否批量的将数据发送给自己。而基于push的系统，由于其并不知道下游consumer的立即处理能力，则其必须选择是单条数据发送，还是积累更多的数据来进行发送。假如需要低延迟的话，则会导致每次只向下游推送一条数据，但这明显比较浪费带宽。而基于pull的系统设计就可以弥补这一缺点，因为consumer总是会拉取当前offset之后的可用消息，因此其可以进行批量优化而不会引入不必要的延迟。

基于pull模式系统的一个缺点在于，假如broker没有数据的话，则consumer可能会陷入一个轮询的死循环中，一直处于处于忙等待状态。为了避免这个问题，在pull请求中我们有相应的参数设置，以允许consumer的请求阻塞在长等待(long-poll waiting)状态，直到有新的数据到达。

你也可以设想一下其他端到端(end-to-end)之间只使用pull模式的设计。例如，producer在本地进行写日志操作，然后broker从producer拉取数据，consumer又从broker拉取数据。一种存储转发(store-and-forward)类型的producer就通常是这样实现的。这种设计看起来也还不错，但仔细考虑其实不是很合理，因为在我们的用例中可能会存在成千上万的producer。根据我们大规模运维持久化数据系统的经验，如果一个系统涉及到成千上万的磁盘，则可能会使得整个系统变得十分的脆弱。在实际使用过程中，我们还发现在大规模SLA（service level agreement)应用中，使用pipeline就可以，而不需要producer具有持久化能力。

### 4.2 consumer消费偏移
跟踪哪些消息已经被消费是消息系统的一个关键性能指标。

大多数消息系统都是在broker上保存哪些消息已经被消费的元数据。即当消息发送给consumer时，broker可能会马上在本地进行记录，或者等待conumser的ack信息。这是一个很直观的选择，并且针对单台服务器来说也确实不太可能将消费偏移保存在其他地方。由于在很多消息系统中，存储所采用的数据结构都没有太好的水平扩展性，因此保存在broker也是一个实用的选择———由于broker知道当前已经消费到什么位置了，因此其就可以删除已消费完的数据，确保不会浪费太多的磁盘空间。

这里有一个隐藏的问题就是，对于消费偏移offset，如何使broker与consumer达成一致呢？假如broker在将消息发送到网络上之后，立马就将该消息标识为```consumed```，则当consumer未能成功处理该消息（例如程序崩溃或超时等）时，则该消息就会被丢失。为了解决这一问题，很多消息系统采用ack机制，这就意味着当broker把消息发送出去时，只是将该消息标记为```sent```，而不是```consumed```状态，然后broker等待consumer对该消息的ack，从而将该消息标志为```consumed```状态。通过这一策略，就避免了消息的丢失，但是却产生了新的问题。首先，假如consumer已经成功的处理了该消息，但是还没来的及ack，consumer程序就崩溃了，那么这条消息就会被消费两次；其次，就是性能方面的问题，现在broker必须要跟踪每一条消息的多个状态（sent状态、consumed状态）。对于一个消息系统，我们必须要处理一些疑难的问题，比如消息已经发送出去，但是没有收到ack。

kafka在处理这一问题时，采用一种不同的方法。在kafka中，topic会被分隔成一个全局有序的partition集合，在任何时间每一个partition都只会被同一个consumer group中的一个consumer所消费。这就意味着在每一个partition中，我们仅仅使用一个整数就可以表示该consumer下一次所消费的位置。这就使得消费状态的表示十分轻巧，每一个partition一个整数即可。我们可以周期性的对该状态进行检查，这就等价于对消息进行ack。

采用此种方法来处理消费偏移还带来了另外一个好处，即consumer可以自由的将offset进行重置，从而重新消费原来的数据。这一点与我们常见的消息队列不一致，但对很多conusmers来说可能确实很有用处。例如，假设consumer的代码存在bug，在消费掉一些消息之后该bug被发现，则在bug修正后可以重新对这些消息进行消费。

### 4.3 离线数据加载
大规模存储要求能够支持consumers间断性的消费消息，例如某个时刻hadoop批量的将数据加载到某个离线系统。

在这种情况下，Hadoop会并发的来进行数据加载，其可以将加载任务分隔成单独的map tasks，之后每一个task对应一个topic/partition，这样就可以实现完全并发。Hadoop提供了task管理功能，当任务失败时，其只需要重新启动任务并从原位置加载数据即可。

### 4.4 静态成员关系
静态成员关系(static membership)的目标是为了提高流应用程序的可用性，消费组(consumer groups)以及其他应用程序构建于```组平衡协议```(group reblance protocol)之上的。重平衡协议依赖于组协调器(group cordinator)来为每一个组成员分配一个ID。组协调器所分配的ID只是暂时性的，当组成员重启(restart)并重新加入组时，其ID会发生改变。对基于consumer的应用程序来说，这种动态的成员关系(dynamic membership)，在进行管理操作时（例如重新部署应用程序、更新配置、或者重启），可能会有大批的任务重新指定到一个不同的实例。对于具有大量状态的应用程序来说，这种任务的变动可能需要花费很长的时间来恢复其本地状态，之后才能够正常工作，则就会导致应用程序可能在一段时间内不能向外提供正常服务。正是注意到了此方面的问题，kafka的组管理协议(group management protocol)允许为组成员提供持久化的实体ID(entity ID)。通过基于这些持久化的ID，组成员关系就能够维持不变，这样组不会触发重平衡。

假如你想要使用静态成员关系(static membership):

* 将kafka broker集群及client的版本更新到```2.3```或之后的版本，并确保更新之后的broker所使用的*inter.broker.protocol.version*大于等于```2.3```。

* 对于一个consumer group，将该组内的每一个consumer设置一个唯一的实例ID（通过*ConsumerConfig#GROUP_INSTANCE_ID_CONFIG*配置项来进行设置）

* 对于kafka流应用，最好是针对每一个KafkaStream实例设置一个唯一的ID（通过*ConsumerConfig#GROUP_INSTANCE_ID_CONFIG*配置项来设置）

假如broker的版本低于```2.3```，但是你在client端通过*ConsumerConfig#GROUP_INSTANCE_ID_CONFIG*设置了唯一的ID，则应用程序会侦测到broker的版本，并抛出UnsupportException。另外，假如碰巧两个不同的实例设置了相同的ID，则在broker端会启用相应的规避机制，并通过触发*org.apache.kafka.common.errors.FencedInstanceIdException*来通知客户端马上关闭程序。

## 5. 消息投递机制
现在我们初步了解了producer与consumer是如何工作的了，现在我们来介绍一下kafka是如何处理producer与consumer之间消息的可靠性？很明显，有多种消息投递策略：

* 至多一次(at most once): 消息可能丢失，绝不重新投递

* 至少一次(at least once): 消息不丢失，但可能会被重新投递

* 投递一次(exactly once): 这是大部分人所期望实现的，每一条消息仅仅只会投递一次







<pre>
</pre>



<br />
<br />

**[参考]**



1. [kafka官网](https://kafka.apache.org/)

2. [历史性难题——如何为Kafka挑选合适的分区数？](https://www.jianshu.com/p/fa7a65febcc0)


<br />
<br />
<br />

